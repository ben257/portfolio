<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Autoencoder on Benjamin Kurland</title><link>https://ben257.github.io/portfolio/tags/autoencoder/</link><description>Recent content in Autoencoder on Benjamin Kurland</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 22 Jun 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://ben257.github.io/portfolio/tags/autoencoder/index.xml" rel="self" type="application/rss+xml"/><item><title>Autoencoder</title><link>https://ben257.github.io/portfolio/posts/autoencoder/</link><pubDate>Sun, 22 Jun 2025 00:00:00 +0000</pubDate><guid>https://ben257.github.io/portfolio/posts/autoencoder/</guid><description>&lt;h3 id="background"&gt;Background&lt;/h3&gt;
&lt;p&gt;An autoencoder is a type of unsupervised deep learning, where the neural network learns a latent representation of the data. There are two parts of the autoencoder: the encoder and decoder (see architecture below). The encoder maps the input data into a latent space. The decoder tries to recreate the input from the encoded representation. The target the autoencoder is trained on is the input.&lt;/p&gt;
&lt;img src="https://ben257.github.io/portfolio/images/ae_architecture.png" alt="alt text" height="400"&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h3 id="reconstructions"&gt;Reconstructions&lt;/h3&gt;
&lt;p&gt;We train an autoencoder on the MNIST data set, with the dimension of the latent space being 2 to allow us to visualize it. After training for five epochs, we see that the autoencoder is doing a decent job of reconstructing images from the validation set. It is able to recreate digits 1, 7, and 0. However, the autoencoder gets some digits confused; it transforms a 2 into a 3; a 4 into a 9, and a 5 into a 6.&lt;/p&gt;</description></item></channel></rss>